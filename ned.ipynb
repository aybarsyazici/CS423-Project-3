{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading anchor candidate data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading item_id to description embedding data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia title embedings...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia items...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytorch_utils\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is cuda available?\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading all-MiniLM-L6-v2 model & Generating sentence embeddings of train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7375/7375 [00:04<00:00, 1511.29it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c8366ebb154c4ebbc29d1274d7c250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating Doc embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [00:07<00:00, 124.79it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4e3a929ab246ae8779d1304c31441d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc61cac26f854cee8ef3d0b889ea9016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading all-MiniLM-L6-v2 model & Generating sentence embeddings of test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3501/3501 [00:02<00:00, 1740.33it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d2c860d3384105a801eedc88147246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating Doc embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:02<00:00, 218.60it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5d730dd8114fd2970af42d0af6f796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbdb2c4230c42bcaf8a552653a23abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pytorch_utils.EntityDataset(device='cuda')\n",
    "test_dataset = pytorch_utils.EntityDataset(train=False, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "temp_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in dataframe called doc_id which is the index of the document\n",
    "# we will do it buy cumsum() the token column. If the value in the token column contains the string -DOCSTART-,\n",
    "# Then we know we are in a new document\n",
    "train['doc_id'] = (train['token'].str.contains('-DOCSTART-', case=True, na=False)).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-DOCSTART- (1 EU)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>EU</td>\n",
       "      <td>B</td>\n",
       "      <td>EU</td>\n",
       "      <td>--NME--</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>rejects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>German</td>\n",
       "      <td>B</td>\n",
       "      <td>German</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Germany</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>call</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218500</th>\n",
       "      <td>218501</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>B</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Swansea_City_A.F.C.</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218501</th>\n",
       "      <td>218502</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218502</th>\n",
       "      <td>218503</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>B</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Lincoln_City_F.C.</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218503</th>\n",
       "      <td>218504</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218504</th>\n",
       "      <td>218505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218505 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id              token entity_tag full_mention  \\\n",
       "0            1  -DOCSTART- (1 EU)        NaN          NaN   \n",
       "1            2                 EU          B           EU   \n",
       "2            3            rejects        NaN          NaN   \n",
       "3            4             German          B       German   \n",
       "4            5               call        NaN          NaN   \n",
       "...        ...                ...        ...          ...   \n",
       "218500  218501            Swansea          B      Swansea   \n",
       "218501  218502                  1        NaN          NaN   \n",
       "218502  218503            Lincoln          B      Lincoln   \n",
       "218503  218504                  2        NaN          NaN   \n",
       "218504  218505                NaN        NaN          NaN   \n",
       "\n",
       "                                                wiki_url  doc_id  \n",
       "0                                                    NaN       1  \n",
       "1                                                --NME--       1  \n",
       "2                                                    NaN       1  \n",
       "3                   http://en.wikipedia.org/wiki/Germany       1  \n",
       "4                                                    NaN       1  \n",
       "...                                                  ...     ...  \n",
       "218500  http://en.wikipedia.org/wiki/Swansea_City_A.F.C.     946  \n",
       "218501                                               NaN     946  \n",
       "218502    http://en.wikipedia.org/wiki/Lincoln_City_F.C.     946  \n",
       "218503                                               NaN     946  \n",
       "218504                                               NaN     946  \n",
       "\n",
       "[218505 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\ned.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m not_nme \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mwiki_url\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m--NME--\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Now for each row that is not_nan & not_nme, we check the token column. If the token column has any previous token with the same value and same doc_id, then we set the full_mention column to be the same as the previous row. Otherwise, we set the full_mention column to be the same as the token column\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mfull_mention\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[not_nan \u001b[39m&\u001b[39;49m not_nme]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: x[\u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mif\u001b[39;49;00m train\u001b[39m.\u001b[39;49mloc[(train[\u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m x[\u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m&\u001b[39;49m (train[\u001b[39m'\u001b[39;49m\u001b[39mdoc_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m x[\u001b[39m'\u001b[39;49m\u001b[39mdoc_id\u001b[39;49m\u001b[39m'\u001b[39;49m]), \u001b[39m'\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mcount() \u001b[39m>\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39melse\u001b[39;49;00m x[\u001b[39m'\u001b[39;49m\u001b[39mfull_mention\u001b[39;49m\u001b[39m'\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10022\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10024\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10025\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10026\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10032\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10033\u001b[0m )\n\u001b[1;32m> 10034\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 963\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    965\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    978\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 979\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    980\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    981\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    982\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    983\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\ned.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m not_nme \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mwiki_url\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m--NME--\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Now for each row that is not_nan & not_nme, we check the token column. If the token column has any previous token with the same value and same doc_id, then we set the full_mention column to be the same as the previous row. Otherwise, we set the full_mention column to be the same as the token column\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39mfull_mention\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[not_nan \u001b[39m&\u001b[39m not_nme]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m train\u001b[39m.\u001b[39mloc[(train[\u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m x[\u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m&\u001b[39m (train[\u001b[39m'\u001b[39;49m\u001b[39mdoc_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m x[\u001b[39m'\u001b[39;49m\u001b[39mdoc_id\u001b[39;49m\u001b[39m'\u001b[39;49m]), \u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcount() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m x[\u001b[39m'\u001b[39m\u001b[39mfull_mention\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\series.py:5799\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5796\u001b[0m lvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[0;32m   5797\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 5799\u001b[0m res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[0;32m   5801\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:349\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    346\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[0;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:220\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m     func \u001b[39m=\u001b[39m partial(expressions\u001b[39m.\u001b[39mevaluate, op)\n\u001b[0;32m    219\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (\n\u001b[0;32m    223\u001b[0m         left\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(right, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m\n\u001b[0;32m    224\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m op_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m _TEST_MODE:\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "not_nan = train['wiki_url'].notna()\n",
    "not_nme = train['wiki_url'] != '--NME--'\n",
    "\n",
    "# Now for each row that is not_nan & not_nme, we check the token column. If the token column has any previous token with the same value and same doc_id, then we set the full_mention column to be the same as the previous row. Otherwise, we set the full_mention column to be the same as the token column\n",
    "train['full_mention'] = train[not_nan & not_nme].apply(lambda x: x['token'] if train.loc[(train['token'] == x['token']) & (train['doc_id'] == x['doc_id']), 'token'].count() > 1 else x['full_mention'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings = temp_model.encode(wiki_items['wikipedia_title'].to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aybar\\AppData\\Local\\Temp\\ipykernel_123484\\2787295153.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  corpus_embeddings = torch.tensor(corpus_embeddings).to('cuda')\n",
      "C:\\Users\\aybar\\AppData\\Local\\Temp\\ipykernel_123484\\2787295153.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_embedding = torch.tensor(query_embedding).to('cuda')\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "query = \"Costas Skandalidis\"\n",
    "\n",
    "query_embedding = temp_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "corpus_embeddings = torch.tensor(corpus_embeddings).to('cuda')\n",
    "query_embedding = torch.tensor(query_embedding).to('cuda')\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3, score_function=util.dot_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_train(x, device=device))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_test(x, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorch_utils.EntityClassifier(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]c:\\Users\\aybar\\Documents\\CS423-Project-3\\pytorch_utils.py:225: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  document_embed = torch.tensor(document_embed, dtype=torch.float32, device=device)\n",
      "Epoch 19 loss: 0.10277010971139726: 100%|██████████| 20/20 [06:22<00:00, 19.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "from torch import optim\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "pbar = tqdm(range(epochs))\n",
    "model.train()\n",
    "for epoch in pbar:\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    pbar.set_description(f'Epoch {epoch} loss: {running_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:08, 16.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's do predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    prediction_indexes = []\n",
    "    for i, data in tqdm(enumerate(test_dataloader)):\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings = data\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        # which index has the highest value?\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_indexes.extend(predicted.tolist())\n",
    "        # predicted is shape (batch_size)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e the candidate_ids is of shape (batch_size, 5)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e get it to shape (batch_size)\n",
    "        best_candidates = candidate_ids[torch.arange(candidate_ids.size(0)), predicted]\n",
    "\n",
    "        # Append the best candidates to the predictions list\n",
    "        predictions.extend(best_candidates.tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=False, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_train(x, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "286it [00:16, 17.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's do predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_train = []\n",
    "    prediction_train_indexes = []\n",
    "    for i, data in tqdm(enumerate(train_dataloader)):\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings, labels = data\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        # which index has the highest value?\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_indexes.extend(predicted.tolist())\n",
    "        # predicted is shape (batch_size)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e the candidate_ids is of shape (batch_size, 5)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e get it to shape (batch_size)\n",
    "        best_candidates = candidate_ids[torch.arange(candidate_ids.size(0)), predicted]\n",
    "\n",
    "        # Append the best candidates to the predictions list\n",
    "        predictions_train.extend(best_candidates.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_from_statistics = []\n",
    "\n",
    "# for i, row in tqdm(test_dataset.entity_df.iterrows()):\n",
    "#     full_mention = row['full_mention'].strip().lower()\n",
    "#     if full_mention in pytorch_utils.anchor_to_candidate:\n",
    "#         best_candidate = pytorch_utils.anchor_to_candidate[full_mention][0]\n",
    "#     else:\n",
    "#         best_candidate = 0\n",
    "#     predictions_from_statistics.append(best_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_items = pd.read_csv(DATA_DIR + 'wiki_lite/wiki_items.csv')\n",
    "# index wiki_items by id\n",
    "wiki_items = wiki_items.set_index('item_id')\n",
    "# Create item_id to wikipedia_title map\n",
    "item_id_to_title = wiki_items['wikipedia_title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_redirects = pd.read_csv(DATA_DIR + 'wiki_lite/enwiki_redirects.tsv', sep='\\t', header=None, names=['source', 'target'])\n",
    "# index enwiki_redirects by source\n",
    "enwiki_redirects = enwiki_redirects.set_index('source')\n",
    "# create source to target map\n",
    "source_to_target = enwiki_redirects['target'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9166/9166 [00:00<00:00, 1406695.59it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_urls = []\n",
    "# Now we will map these into wikipedia_urls\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    if predictions[i] == 0:\n",
    "        # if the prediction is 0, we will append a blank url\n",
    "        wiki_urls.append('NOT_FOUND')\n",
    "        continue\n",
    "    wikipedia_title = item_id_to_title[predictions[i]]\n",
    "    # does this wikipedia title exist in the redirects?\n",
    "    if wikipedia_title in source_to_target:\n",
    "        # if it does, we will replace it with the redirect\n",
    "        new_title = source_to_target[wikipedia_title]\n",
    "    # Now replace the spaces with underscores\n",
    "    wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "    # And add the wikipedia url\n",
    "    wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18288/18288 [00:00<00:00, 1685685.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_wiki_urls = []\n",
    "# Now we will map these into wikipedia_urls\n",
    "for i in tqdm(range(len(predictions_train))):\n",
    "    if predictions_train[i] == 0:\n",
    "        # if the prediction is 0, we will append a blank url\n",
    "        train_wiki_urls.append('NOT_FOUND')\n",
    "        continue\n",
    "    wikipedia_title = item_id_to_title[predictions_train[i]]\n",
    "    # does this wikipedia title exist in the redirects?\n",
    "    if wikipedia_title in source_to_target:\n",
    "        # if it does, we will replace it with the redirect\n",
    "        new_title = source_to_target[wikipedia_title]\n",
    "    # Now replace the spaces with underscores\n",
    "    wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "    # And add the wikipedia url\n",
    "    train_wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention wiki_url\n",
       "65002  65002  Dejan          B  Dejan Koturovic        ?"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_nan = test['wiki_url'].notna()\n",
    "not_nme = test['wiki_url'] != '--NME--'\n",
    "train_not_nan = train['wiki_url'].notna()\n",
    "train_not_nme = train['wiki_url'] != '--NME--'\n",
    "test.loc[(not_nan & not_nme) & (test.id == 65002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[not_nan & not_nme, 'wiki_url'] = wiki_urls\n",
    "train.loc[train_not_nan & train_not_nme, 'wiki_url'] = train_wiki_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN or --NME-- with NOT_FOUND\n",
    "test['wiki_url'] = test['wiki_url'].fillna('NOT_FOUND')\n",
    "test['wiki_url'] = test['wiki_url'].replace('--NME--', 'NOT_FOUND')\n",
    "train['wiki_url'] = train['wiki_url'].fillna('NOT_FOUND')\n",
    "train['wiki_url'] = train['wiki_url'].replace('--NME--', 'NOT_FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Dejan_Koturović</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention  \\\n",
       "65002  65002  Dejan          B  Dejan Koturovic   \n",
       "\n",
       "                                           wiki_url  \n",
       "65002  http://en.wikipedia.org/wiki/Dejan_Koturović  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.id == 65002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-DOCSTART- (947testa CRICKET) CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .  LONDON 1996-08-30  West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .  Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .  After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .  Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .  Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .  Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .  By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test.token.fillna('', inplace=False).to_list()[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One problem, Simmons does not retrieve the cricket player, but querying for Phil Simmons does.\n",
    "# # TODO: Fix this, one idea is to group by per doc id, check if this token has a better previous full mention\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['simmons'])])\n",
    "\n",
    "# print('____')\n",
    "\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['phil simmons'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a .csv file from id, wiki_url\n",
    "test[['id', 'wiki_url']].to_csv('submission_with_doc.csv', index=False)\n",
    "train[['id', 'wiki_url']].to_csv('train_with_doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
