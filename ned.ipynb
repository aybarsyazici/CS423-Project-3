{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading anchor candidate data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading item_id to statement embedding data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading item_id to description embedding data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia title embedings...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia items...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytorch_utils\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is cuda available?\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading all-MiniLM-L6-v2 model & Generating sentence embeddings of train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7375/7375 [00:02<00:00, 2816.78it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf080675a15483aa02f17b499e9f542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating Doc embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 946/946 [00:04<00:00, 204.45it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2142b675c2d84d4d91332759565c8caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33380990c69147499fdb2566523d9b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading all-MiniLM-L6-v2 model & Generating sentence embeddings of test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3501/3501 [00:01<00:00, 2769.46it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d7bd9fda7c41e0afade7067b6b6eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating Doc embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:01<00:00, 357.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e9bed121dc4963bc3080872ea53158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d461974cb79450a85fb79140359c3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pytorch_utils.EntityDataset(device='cuda')\n",
    "test_dataset = pytorch_utils.EntityDataset(train=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_train(x, device=device))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_test(x, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorch_utils.EntityClassifier(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]c:\\Users\\aybar\\Documents\\CS423-Project-3\\pytorch_utils.py:255: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  document_embed = torch.tensor(document_embed, dtype=torch.float32, device=device)\n",
      "Epoch 19 loss: 0.04415383163458583 Avg shape: 55.32517482517483: 100%|██████████| 20/20 [06:46<00:00, 20.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "from torch import optim\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "pbar = tqdm(range(epochs))\n",
    "model.train()\n",
    "for epoch in pbar:\n",
    "    running_loss = 0.0\n",
    "    shapes = []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings, labels = data\n",
    "        shapes.append(document_embeds.shape[0])\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    pbar.set_description(f'Epoch {epoch} loss: {running_loss / len(dataloader)} Avg shape: {sum(shapes) / len(shapes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:11, 12.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's do predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    prediction_indexes = []\n",
    "    for i, data in tqdm(enumerate(test_dataloader)):\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings = data\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        # which index has the highest value?\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_indexes.extend(predicted.tolist())\n",
    "        # predicted is shape (batch_size)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e the candidate_ids is of shape (batch_size, 5)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e get it to shape (batch_size)\n",
    "        best_candidates = candidate_ids[torch.arange(candidate_ids.size(0)), predicted]\n",
    "\n",
    "        # Append the best candidates to the predictions list\n",
    "        predictions.extend(best_candidates.tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_from_statistics = []\n",
    "\n",
    "# for i, row in tqdm(test_dataset.entity_df.iterrows()):\n",
    "#     full_mention = row['full_mention'].strip().lower()\n",
    "#     if full_mention in pytorch_utils.anchor_to_candidate:\n",
    "#         best_candidate = pytorch_utils.anchor_to_candidate[full_mention][0]\n",
    "#     else:\n",
    "#         best_candidate = 0\n",
    "#     predictions_from_statistics.append(best_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_items = pd.read_csv(DATA_DIR + 'wiki_lite/wiki_items.csv')\n",
    "# index wiki_items by id\n",
    "wiki_items = wiki_items.set_index('item_id')\n",
    "# Create item_id to wikipedia_title map\n",
    "item_id_to_title = wiki_items['wikipedia_title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_redirects = pd.read_csv(DATA_DIR + 'wiki_lite/enwiki_redirects.tsv', sep='\\t', header=None, names=['source', 'target'])\n",
    "# index enwiki_redirects by source\n",
    "enwiki_redirects = enwiki_redirects.set_index('source')\n",
    "# create source to target map\n",
    "source_to_target = enwiki_redirects['target'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9166/9166 [00:00<00:00, 963726.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9138 wikipedia urls\n",
      "Not found 28 wikipedia urls\n",
      "Percentage of wikipedia urls found: 0.9969452323805368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_urls = []\n",
    "not_found = 0\n",
    "found = 0\n",
    "# Now we will map these into wikipedia_urls\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    if predictions[i] == 0:\n",
    "        # if the prediction is 0, we will append a blank url\n",
    "        wiki_urls.append('NOT_FOUND')\n",
    "        not_found += 1\n",
    "        continue\n",
    "    wikipedia_title = item_id_to_title[predictions[i]]\n",
    "    # does this wikipedia title exist in the redirects?\n",
    "    if wikipedia_title in source_to_target:\n",
    "        # if it does, we will replace it with the redirect\n",
    "        new_title = source_to_target[wikipedia_title]\n",
    "    # Now replace the spaces with underscores\n",
    "    wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "    # And add the wikipedia url\n",
    "    wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')\n",
    "    found += 1\n",
    "\n",
    "print(f'Found {found} wikipedia urls')\n",
    "print(f'Not found {not_found} wikipedia urls')\n",
    "print(f'Percentage of wikipedia urls found: {found / (found + not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_found = 0\n",
    "# found = 0\n",
    "# train_wiki_urls = []\n",
    "# # Now we will map these into wikipedia_urls\n",
    "# for i in tqdm(range(len(predictions_train))):\n",
    "#     if predictions_train[i] == 0:\n",
    "#         # if the prediction is 0, we will append a blank url\n",
    "#         train_wiki_urls.append('NOT_FOUND')\n",
    "#         not_found += 1\n",
    "#         continue\n",
    "#     wikipedia_title = item_id_to_title[predictions_train[i]]\n",
    "#     # does this wikipedia title exist in the redirects?\n",
    "#     if wikipedia_title in source_to_target:\n",
    "#         # if it does, we will replace it with the redirect\n",
    "#         new_title = source_to_target[wikipedia_title]\n",
    "#     # Now replace the spaces with underscores\n",
    "#     wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "#     # And add the wikipedia url\n",
    "#     train_wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')\n",
    "#     found += 1\n",
    "\n",
    "# print(f'Found {found} wikipedia urls')\n",
    "# print(f'Not found {not_found} wikipedia urls')\n",
    "# print(f'Percentage of wikipedia urls found: {found / (found + not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "# train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention wiki_url\n",
       "65002  65002  Dejan          B  Dejan Koturovic        ?"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_nan = test['wiki_url'].notna()\n",
    "not_nme = test['wiki_url'] != '--NME--'\n",
    "# train_not_nan = train['wiki_url'].notna()\n",
    "# train_not_nme = train['wiki_url'] != '--NME--'\n",
    "test.loc[(not_nan & not_nme) & (test.id == 65002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[not_nan & not_nme, 'wiki_url'] = wiki_urls\n",
    "# train.loc[train_not_nan & train_not_nme, 'wiki_url'] = train_wiki_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN or --NME-- with NOT_FOUND\n",
    "test['wiki_url'] = test['wiki_url'].fillna('NOT_FOUND')\n",
    "test['wiki_url'] = test['wiki_url'].replace('--NME--', 'NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].fillna('NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].replace('--NME--', 'NOT_FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Dejan_Koturović</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention  \\\n",
       "65002  65002  Dejan          B  Dejan Koturovic   \n",
       "\n",
       "                                           wiki_url  \n",
       "65002  http://en.wikipedia.org/wiki/Dejan_Koturović  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.id == 65002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-DOCSTART- (947testa CRICKET) CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .  LONDON 1996-08-30  West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .  Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .  After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .  Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .  Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .  Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .  By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test.token.fillna('', inplace=False).to_list()[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One problem, Simmons does not retrieve the cricket player, but querying for Phil Simmons does.\n",
    "# # TODO: Fix this, one idea is to group by per doc id, check if this token has a better previous full mention\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['simmons'])])\n",
    "\n",
    "# print('____')\n",
    "\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['phil simmons'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a .csv file from id, wiki_url\n",
    "test[['id', 'wiki_url']].to_csv('submission_with_doc_lower_score_limit_sample_skip.csv', index=False)\n",
    "# train[['id', 'wiki_url']].to_csv('train_with_doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
