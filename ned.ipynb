{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading anchor candidate data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia title embedings...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading KB explanations...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia items...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytorch_utils\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is cuda available?\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading train set...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading train_context_text_150.pkl from data/pkl...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493db297dd674360a62c56eb45ca2e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Length: 18288\n",
      "Entity shape: torch.Size([18288, 384])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9dc569ee184753accdd2e3f8df787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now computing syntax candidates for each entity...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now computing OLD syntax candidates for each entity...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating inputs and labels...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading train_batch_inputs_input_ids.pt, train_batch_inputs_attention_mask.pt, train_batch_entity_df_indexes.pkl, train_batch_labels.pkl, train_batch_candidate_ids.pkl from data/pkl...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading test set...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading test_context_text_150.pkl from data/pkl...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a16d7c19714e1696c3f70ccad161d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Length: 9166\n",
      "Entity shape: torch.Size([9166, 384])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d803de5bd727456ea68120cf4cbbeb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now computing syntax candidates for each entity...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now computing OLD syntax candidates for each entity...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating inputs and labels...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading test_batch_inputs_input_ids.pt, test_batch_inputs_attention_mask.pt, test_batch_entity_df_indexes.pkl, test_batch_candidate_ids.pkl from data/pkl...\n"
     ]
    }
   ],
   "source": [
    "dataset = pytorch_utils.EntityDataset(device='cuda')\n",
    "test_dataset = pytorch_utils.EntityDataset(train=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Deleting corpus embeddings...\n"
     ]
    }
   ],
   "source": [
    "pytorch_utils.delete_corpus_embeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# garbage collect\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_train(x, device=device))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_test(x, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorch_utils.EntityClassifier(transformer_model='distilbert-base-uncased', hidden_size=256, device=device)\n",
    "# does distilbert_model.pt exists? If so load the weights\n",
    "import os\n",
    "if os.path.exists('distilbert_epoch8_0.0578.pt'):\n",
    "    model.load_state_dict(torch.load('distilbert_epoch8_0.0578.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.transformer.layer.5.attention.q_lin.weight\n",
      "transformer.transformer.layer.5.attention.q_lin.bias\n",
      "transformer.transformer.layer.5.attention.k_lin.weight\n",
      "transformer.transformer.layer.5.attention.k_lin.bias\n",
      "transformer.transformer.layer.5.attention.v_lin.weight\n",
      "transformer.transformer.layer.5.attention.v_lin.bias\n",
      "transformer.transformer.layer.5.attention.out_lin.weight\n",
      "transformer.transformer.layer.5.attention.out_lin.bias\n",
      "transformer.transformer.layer.5.sa_layer_norm.weight\n",
      "transformer.transformer.layer.5.sa_layer_norm.bias\n",
      "transformer.transformer.layer.5.ffn.lin1.weight\n",
      "transformer.transformer.layer.5.ffn.lin1.bias\n",
      "transformer.transformer.layer.5.ffn.lin2.weight\n",
      "transformer.transformer.layer.5.ffn.lin2.bias\n",
      "transformer.transformer.layer.5.output_layer_norm.weight\n",
      "transformer.transformer.layer.5.output_layer_norm.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Freeze everything except for:\n",
    "transformer.transformer.layer.5.attention.q_lin.weight\n",
    "transformer.transformer.layer.5.attention.q_lin.bias\n",
    "transformer.transformer.layer.5.attention.k_lin.weight\n",
    "transformer.transformer.layer.5.attention.k_lin.bias\n",
    "transformer.transformer.layer.5.attention.v_lin.weight\n",
    "transformer.transformer.layer.5.attention.v_lin.bias\n",
    "transformer.transformer.layer.5.attention.out_lin.weight\n",
    "transformer.transformer.layer.5.attention.out_lin.bias\n",
    "transformer.transformer.layer.5.sa_layer_norm.weight\n",
    "transformer.transformer.layer.5.sa_layer_norm.bias\n",
    "transformer.transformer.layer.5.ffn.lin1.weight\n",
    "transformer.transformer.layer.5.ffn.lin1.bias\n",
    "transformer.transformer.layer.5.ffn.lin2.weight\n",
    "transformer.transformer.layer.5.ffn.lin2.bias\n",
    "transformer.transformer.layer.5.output_layer_norm.weight\n",
    "transformer.transformer.layer.5.output_layer_norm.bias\n",
    "classifier.0.weight\n",
    "classifier.0.bias\n",
    "classifier.3.weight\n",
    "classifier.3.bias\n",
    "\n",
    "Basically, we want to fine-tune the last layer of the transformer and the classifier\n",
    "\"\"\"\n",
    "for name, param in model.named_parameters():\n",
    "    if 'transformer' in name:\n",
    "        if 'layer.5' in name:\n",
    "            param.requires_grad = True\n",
    "        elif 'pooler' in name:\n",
    "            param.requires_grad = True\n",
    "        elif 'layer.4' in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    elif 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check if all parameters are frozen except for the ones we want\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "from torch import optim\n",
    "# Since for each mention, there are 16 candidates, we need to use CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "desc = f'Epoch {1} loss: ?? Avg shape: ??'\n",
    "epochs = 0\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_loss_divider = 0\n",
    "    running_loss_divided_formatted = 0\n",
    "    if epoch == epochs - 1:\n",
    "        pbar2 = tqdm(enumerate(dataloader), leave=True, total=len(dataloader), desc=desc)\n",
    "    else:\n",
    "        pbar2 = tqdm(enumerate(dataloader), leave=False, total=len(dataloader), desc=desc)\n",
    "    for i, data in pbar2:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        tokenized_inputs_input_ids, tokenized_inputs_attention_mask, index, label = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(tokenized_inputs_input_ids, tokenized_inputs_attention_mask)\n",
    "        # Output shape is: (batch_size, 16), i.e one prob/score for each candidate(16 candidates)\n",
    "        # label shape is: (batch_size), the index of the correct candidate\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_loss_divider += 1\n",
    "        running_loss_divided_formatted = f'{(running_loss / running_loss_divider):.4f}'\n",
    "        desc = f'Epoch {epoch+1} loss: {running_loss_divided_formatted}'\n",
    "        pbar2.set_description(desc)\n",
    "        pbar2.update(1)\n",
    "    # save the model\n",
    "    torch.save(model.state_dict(), f'models/distilbert_epoch{epoch+1}_{running_loss_divided_formatted}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1146/1146 [10:16<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Put the model in evaluation mode\n",
    "final_disambiguations = {}  # Initialize a dictionary to hold results\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        tokenized_input_id_batch, tokenized_attention_mask_batch, indexes, candidate_ids_batch = batch\n",
    "        probabilities = model(tokenized_inputs_input_ids, tokenized_inputs_attention_mask)\n",
    "        # probs is of shape (batch_size, 16)\n",
    "        # candidate_ids_batch is of shape (batch_size, 16)\n",
    "        # Grab the index of the highest probability for each batch\n",
    "        # This is the index of the correct candidate\n",
    "        _, indices = torch.max(probabilities, 1)\n",
    "        # indices is of shape (batch_size)\n",
    "        # candidate_ids_batch is of shape (batch_size, 16)\n",
    "        # Grab the correct candidate for each batch\n",
    "        final_disambiguations.update({index.detach().cpu().item(): candidate_ids_batch[i][indices[i]].detach().cpu().item() for i, index in enumerate(indexes)})\n",
    "        # final_disambiguations is a dictionary of shape (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_items = pd.read_csv(DATA_DIR + 'wiki_lite/wiki_items.csv')\n",
    "# index wiki_items by id\n",
    "wiki_items = wiki_items.set_index('item_id')\n",
    "# Create item_id to wikipedia_title map\n",
    "item_id_to_title = wiki_items['wikipedia_title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_redirects = pd.read_csv(DATA_DIR + 'wiki_lite/enwiki_redirects.tsv', sep='\\t', header=None, names=['source', 'target'])\n",
    "# index enwiki_redirects by source\n",
    "enwiki_redirects = enwiki_redirects.set_index('source')\n",
    "# create source to target map\n",
    "source_to_target = enwiki_redirects['target'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9166/9166 [00:00<00:00, 850289.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6894 wikipedia urls\n",
      "Not found 0 wikipedia urls\n",
      "Percentage of wikipedia urls found: 1.0\n",
      "Percentage of wikipedia urls redirected: 0.0013054830287206266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_urls = []\n",
    "not_found = 0\n",
    "found = 0\n",
    "redirection = 0\n",
    "for i in tqdm(range(test_dataset.entity_df.shape[0])):\n",
    "    if i in final_disambiguations:\n",
    "        candidate = final_disambiguations[i]\n",
    "        if candidate == -1:\n",
    "            wiki_urls.append('NOT_FOUND')\n",
    "            continue\n",
    "        wikipedia_title = item_id_to_title[candidate]\n",
    "        # does this wikipedia title exist in the redirects?\n",
    "        if wikipedia_title in source_to_target:\n",
    "            # if it does, we will replace it with the redirect\n",
    "            wikipedia_title = source_to_target[wikipedia_title]\n",
    "            redirection += 1\n",
    "        # Now replace the spaces with underscores\n",
    "        wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "        # And add the wikipedia url\n",
    "        wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')\n",
    "        found += 1\n",
    "    else:\n",
    "        wiki_urls.append('NOT_FOUND')\n",
    "        not_found += 1\n",
    "\n",
    "print(f'Found {found} wikipedia urls')\n",
    "print(f'Not found {not_found} wikipedia urls')\n",
    "print(f'Percentage of wikipedia urls found: {found / (found + not_found)}')\n",
    "print(f'Percentage of wikipedia urls redirected: {redirection / found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation prepped csv\n",
    "validation_prepped = pd.read_csv(DATA_DIR + 'validation_prepped.csv')\n",
    "true_wiki_urls = validation_prepped['2'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14673794457778747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(true_wiki_urls, wiki_urls, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "# train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention wiki_url\n",
       "65002  65002  Dejan          B  Dejan Koturovic        ?"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_nan = test['wiki_url'].notna()\n",
    "not_nme = test['wiki_url'] != '--NME--'\n",
    "# train_not_nan = train['wiki_url'].notna()\n",
    "# train_not_nme = train['wiki_url'] != '--NME--'\n",
    "test.loc[(not_nan & not_nme) & (test.id == 65002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[not_nan & not_nme, 'wiki_url'] = wiki_urls\n",
    "# train.loc[train_not_nan & train_not_nme, 'wiki_url'] = train_wiki_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN or --NME-- with NOT_FOUND\n",
    "test['wiki_url'] = test['wiki_url'].fillna('NOT_FOUND')\n",
    "test['wiki_url'] = test['wiki_url'].replace('--NME--', 'NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].fillna('NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].replace('--NME--', 'NOT_FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Dejan_Koturović</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention  \\\n",
       "65002  65002  Dejan          B  Dejan Koturovic   \n",
       "\n",
       "                                           wiki_url  \n",
       "65002  http://en.wikipedia.org/wiki/Dejan_Koturović  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.id == 65002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-DOCSTART- (947testa CRICKET) CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .  LONDON 1996-08-30  West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .  Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .  After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .  Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .  Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .  Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .  By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test.token.fillna('', inplace=False).to_list()[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One problem, Simmons does not retrieve the cricket player, but querying for Phil Simmons does.\n",
    "# # TODO: Fix this, one idea is to group by per doc id, check if this token has a better previous full mention\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['simmons'])])\n",
    "\n",
    "# print('____')\n",
    "\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['phil simmons'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a .csv file from id, wiki_url\n",
    "test[['id', 'wiki_url']].to_csv('submission_distilbert_epoch10.csv', index=False)\n",
    "# train[['id', 'wiki_url']].to_csv('train_with_doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
