{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading anchor candidate data...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia title embedings...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading wikipedia items...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pytorch_utils\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is cuda available?\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aybar\\Documents\\CS423-Project-3\\pytorch_utils.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.train_data_no_doc_start['entity_loc'] = self.train_data_no_doc_start.groupby(['doc_id']).cumcount()\n",
      "100%|██████████| 18288/18288 [02:17<00:00, 133.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Saving train_context_text_150.pkl to data/pkl...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adb90133eb94cee881b4b6cffc27c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/572 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Length: 18288\n",
      "Entity shape: torch.Size([18288, 384])\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Loading test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aybar\\Documents\\CS423-Project-3\\pytorch_utils.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.train_data_no_doc_start['entity_loc'] = self.train_data_no_doc_start.groupby(['doc_id']).cumcount()\n",
      "100%|██████████| 9166/9166 [00:38<00:00, 240.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Saving test_context_text_150.pkl to data/pkl...\n",
      "\u001b[32m[PyTorch-Utils]\u001b[0m: Now generating entity embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c58680b9b8447b58530f1854092fc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Length: 9166\n",
      "Entity shape: torch.Size([9166, 384])\n"
     ]
    }
   ],
   "source": [
    "dataset = pytorch_utils.EntityDataset(device='cpu')\n",
    "test_dataset = pytorch_utils.EntityDataset(train=False, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pytorch_utils.corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_train(x, device=device, entity_embed_tensor=dataset.entity_embeddings))\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=lambda x: pytorch_utils.EntityDataset.collate_fn_test(x, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pytorch_utils.EntityClassifier(transformer_model='bert-base-uncased', hidden_size=256, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layer.11.attention.self.query.weight\n",
      "transformer.encoder.layer.11.attention.self.query.bias\n",
      "transformer.encoder.layer.11.attention.self.key.weight\n",
      "transformer.encoder.layer.11.attention.self.key.bias\n",
      "transformer.encoder.layer.11.attention.self.value.weight\n",
      "transformer.encoder.layer.11.attention.self.value.bias\n",
      "transformer.encoder.layer.11.attention.output.dense.weight\n",
      "transformer.encoder.layer.11.attention.output.dense.bias\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "transformer.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "transformer.encoder.layer.11.intermediate.dense.weight\n",
      "transformer.encoder.layer.11.intermediate.dense.bias\n",
      "transformer.encoder.layer.11.output.dense.weight\n",
      "transformer.encoder.layer.11.output.dense.bias\n",
      "transformer.encoder.layer.11.output.LayerNorm.weight\n",
      "transformer.encoder.layer.11.output.LayerNorm.bias\n",
      "transformer.pooler.dense.weight\n",
      "transformer.pooler.dense.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Freeze everything except for:\n",
    "transformer.encoder.layer.11.attention.self.query.weight\n",
    "transformer.encoder.layer.11.attention.self.query.bias\n",
    "transformer.encoder.layer.11.attention.self.key.weight\n",
    "transformer.encoder.layer.11.attention.self.key.bias\n",
    "transformer.encoder.layer.11.attention.self.value.weight\n",
    "transformer.encoder.layer.11.attention.self.value.bias\n",
    "transformer.encoder.layer.11.attention.output.dense.weight\n",
    "transformer.encoder.layer.11.attention.output.dense.bias\n",
    "transformer.encoder.layer.11.attention.output.LayerNorm.weight\n",
    "transformer.encoder.layer.11.attention.output.LayerNorm.bias\n",
    "transformer.encoder.layer.11.intermediate.dense.weight\n",
    "transformer.encoder.layer.11.intermediate.dense.bias\n",
    "transformer.encoder.layer.11.output.dense.weight\n",
    "transformer.encoder.layer.11.output.dense.bias\n",
    "transformer.encoder.layer.11.output.LayerNorm.weight\n",
    "transformer.encoder.layer.11.output.LayerNorm.bias\n",
    "transformer.pooler.dense.weight\n",
    "transformer.pooler.dense.bias\n",
    "classifier.0.weight\n",
    "classifier.0.bias\n",
    "classifier.3.weight\n",
    "classifier.3.bias\n",
    "\n",
    "Basically, we want to fine-tune the last layer of the transformer and the classifier\n",
    "\"\"\"\n",
    "for name, param in model.named_parameters():\n",
    "    if 'transformer' in name:\n",
    "        if'encoder' in name and 'layer.11' in name:\n",
    "            param.requires_grad = True\n",
    "        elif 'pooler' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    elif 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Check if all parameters are frozen except for the ones we want\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: ?? Avg shape: ??:   0%|          | 0/1143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: ?? Avg shape: ??:   0%|          | 5/1143 [00:14<53:21,  2.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\ned.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(candidates_input_ids, candidates_attention_mask, candidates_token_type_ids, contexts_input_ids, contexts_attention_mask, contexts_token_type_ids, full_mentions_input_ids, full_mentions_attention_mask, full_mentions_token_type_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aybar/Documents/CS423-Project-3/ned.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\pytorch_utils.py:303\u001b[0m, in \u001b[0;36mEntityClassifier.forward\u001b[1;34m(self, candidates_input_ids, candidates_attention_mask, candidates_token_type_ids, contexts_input_ids, contexts_attention_mask, contexts_token_type_ids, full_mentions_input_ids, full_mentions_attention_mask, full_mentions_token_type_ids)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39m# Loop over each candidate\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \n\u001b[0;32m    297\u001b[0m \u001b[39m# candidate_input_ids: (batch_size, num_candidates, max_length)\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m# candidate_attention_mask: (batch_size, num_candidates, max_length)\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39m# candidate_token_type_ids: (batch_size, num_candidates, max_length)\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[39m# If we do: candidates_input_ids[:, i], we get the i-th candidate of each sample in the batch, shape: (batch_size, max_length)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_candidates):\n\u001b[0;32m    302\u001b[0m     \u001b[39m# Extract embeddings for the i-th candidate of each sample\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m     candidate_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    304\u001b[0m         input_ids\u001b[39m=\u001b[39;49mcandidates_input_ids[:, i],\n\u001b[0;32m    305\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcandidates_attention_mask[:, i],\n\u001b[0;32m    306\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mcandidates_token_type_ids[:, i]\n\u001b[0;32m    307\u001b[0m     )\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    309\u001b[0m     \u001b[39m# Concatenate embeddings and pass through classifier\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     candidate_combined_embed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((candidate_embeds, combined_embeds), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# shape: (batch_size, 3 * hidden_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1009\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1010\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[0;32m   1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1014\u001b[0m     embedding_output,\n\u001b[0;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1024\u001b[0m )\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aybar\\Documents\\CS423-Project-3\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:238\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    237\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 238\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[0;32m    239\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    240\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "from torch import optim\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "desc = f'Epoch {1} loss: ?? Avg shape: ??'\n",
    "epochs = 20\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    shapes = []\n",
    "    pbar2 = tqdm(enumerate(dataloader), leave=True, total=len(dataloader), desc=desc)\n",
    "    for i, data in pbar2:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        candidates_input_ids, candidates_attention_mask, candidates_token_type_ids, contexts_input_ids, contexts_attention_mask, contexts_token_type_ids, full_mentions_input_ids, full_mentions_attention_mask, full_mentions_token_type_ids, labels = data\n",
    "        shapes.append(candidates_input_ids.shape[0])\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(candidates_input_ids, candidates_attention_mask, candidates_token_type_ids, contexts_input_ids, contexts_attention_mask, contexts_token_type_ids, full_mentions_input_ids, full_mentions_attention_mask, full_mentions_token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        pbar2.update(1)\n",
    "    desc = f'Epoch {epoch+1} loss: {running_loss / len(dataloader)} Avg shape: {sum(shapes) / len(shapes)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings \n",
    "\n",
    "if isinstance(query_embeddings, (np.ndarray, np.generic)):\n",
    "    query_embeddings = torch.from_numpy(query_embeddings)\n",
    "elif isinstance(query_embeddings, list):\n",
    "    query_embeddings = torch.stack(query_embeddings)\n",
    "\n",
    "if len(query_embeddings.shape) == 1:\n",
    "    query_embeddings = query_embeddings.unsqueeze(0)\n",
    "\n",
    "if isinstance(corpus_embeddings, (np.ndarray, np.generic)):\n",
    "    corpus_embeddings = torch.from_numpy(corpus_embeddings)\n",
    "elif isinstance(corpus_embeddings, list):\n",
    "    corpus_embeddings = torch.stack(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:09, 14.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# let's do predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    prediction_indexes = []\n",
    "    for i, data in tqdm(enumerate(test_dataloader)):\n",
    "        document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings = data\n",
    "        outputs = model((document_embeds, sentence_embeddings, entity_embeddings, candidate_ids, candidate_description_embeddings))\n",
    "        # which index has the highest value?\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_indexes.extend(predicted.tolist())\n",
    "        # predicted is shape (batch_size)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e the candidate_ids is of shape (batch_size, 5)\n",
    "        # we want to grab the best candidate for batch item i\n",
    "        # i.e get it to shape (batch_size)\n",
    "        best_candidates = candidate_ids[torch.arange(candidate_ids.size(0)), predicted]\n",
    "\n",
    "        # Append the best candidates to the predictions list\n",
    "        predictions.extend(best_candidates.tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_from_statistics = []\n",
    "\n",
    "# for i, row in tqdm(test_dataset.entity_df.iterrows()):\n",
    "#     full_mention = row['full_mention'].strip().lower()\n",
    "#     if full_mention in pytorch_utils.anchor_to_candidate:\n",
    "#         best_candidate = pytorch_utils.anchor_to_candidate[full_mention][0]\n",
    "#     else:\n",
    "#         best_candidate = 0\n",
    "#     predictions_from_statistics.append(best_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_items = pd.read_csv(DATA_DIR + 'wiki_lite/wiki_items.csv')\n",
    "# index wiki_items by id\n",
    "wiki_items = wiki_items.set_index('item_id')\n",
    "# Create item_id to wikipedia_title map\n",
    "item_id_to_title = wiki_items['wikipedia_title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwiki_redirects = pd.read_csv(DATA_DIR + 'wiki_lite/enwiki_redirects.tsv', sep='\\t', header=None, names=['source', 'target'])\n",
    "# index enwiki_redirects by source\n",
    "enwiki_redirects = enwiki_redirects.set_index('source')\n",
    "# create source to target map\n",
    "source_to_target = enwiki_redirects['target'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9166/9166 [00:00<00:00, 1663205.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8803 wikipedia urls\n",
      "Not found 363 wikipedia urls\n",
      "Percentage of wikipedia urls found: 0.9603971197905302\n",
      "Percentage of wikipedia urls redirected: 0.0029535385663978187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wiki_urls = []\n",
    "not_found = 0\n",
    "found = 0\n",
    "redirection = 0\n",
    "# Now we will map these into wikipedia_urls\n",
    "for i in tqdm(range(len(predictions))):\n",
    "    if predictions[i] == 0:\n",
    "        # if the prediction is 0, we will append a blank url\n",
    "        wiki_urls.append('NOT_FOUND')\n",
    "        not_found += 1\n",
    "        continue\n",
    "    wikipedia_title = item_id_to_title[predictions[i]]\n",
    "    # does this wikipedia title exist in the redirects?\n",
    "    if wikipedia_title in source_to_target:\n",
    "        # if it does, we will replace it with the redirect\n",
    "        wikipedia_title = source_to_target[wikipedia_title]\n",
    "        redirection += 1\n",
    "    # Now replace the spaces with underscores\n",
    "    wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "    # And add the wikipedia url\n",
    "    wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')\n",
    "    found += 1\n",
    "\n",
    "print(f'Found {found} wikipedia urls')\n",
    "print(f'Not found {not_found} wikipedia urls')\n",
    "print(f'Percentage of wikipedia urls found: {found / (found + not_found)}')\n",
    "print(f'Percentage of wikipedia urls redirected: {redirection / found}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_found = 0\n",
    "# found = 0\n",
    "# train_wiki_urls = []\n",
    "# # Now we will map these into wikipedia_urls\n",
    "# for i in tqdm(range(len(predictions_train))):\n",
    "#     if predictions_train[i] == 0:\n",
    "#         # if the prediction is 0, we will append a blank url\n",
    "#         train_wiki_urls.append('NOT_FOUND')\n",
    "#         not_found += 1\n",
    "#         continue\n",
    "#     wikipedia_title = item_id_to_title[predictions_train[i]]\n",
    "#     # does this wikipedia title exist in the redirects?\n",
    "#     if wikipedia_title in source_to_target:\n",
    "#         # if it does, we will replace it with the redirect\n",
    "#         new_title = source_to_target[wikipedia_title]\n",
    "#     # Now replace the spaces with underscores\n",
    "#     wikipedia_title = wikipedia_title.replace(' ', '_')\n",
    "#     # And add the wikipedia url\n",
    "#     train_wiki_urls.append(f'http://en.wikipedia.org/wiki/{wikipedia_title}')\n",
    "#     found += 1\n",
    "\n",
    "# print(f'Found {found} wikipedia urls')\n",
    "# print(f'Not found {not_found} wikipedia urls')\n",
    "# print(f'Percentage of wikipedia urls found: {found / (found + not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "# train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention wiki_url\n",
       "65002  65002  Dejan          B  Dejan Koturovic        ?"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_nan = test['wiki_url'].notna()\n",
    "not_nme = test['wiki_url'] != '--NME--'\n",
    "# train_not_nan = train['wiki_url'].notna()\n",
    "# train_not_nme = train['wiki_url'] != '--NME--'\n",
    "test.loc[(not_nan & not_nme) & (test.id == 65002)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[not_nan & not_nme, 'wiki_url'] = wiki_urls\n",
    "# train.loc[train_not_nan & train_not_nme, 'wiki_url'] = train_wiki_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN or --NME-- with NOT_FOUND\n",
    "test['wiki_url'] = test['wiki_url'].fillna('NOT_FOUND')\n",
    "test['wiki_url'] = test['wiki_url'].replace('--NME--', 'NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].fillna('NOT_FOUND')\n",
    "# train['wiki_url'] = train['wiki_url'].replace('--NME--', 'NOT_FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>entity_tag</th>\n",
       "      <th>full_mention</th>\n",
       "      <th>wiki_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65002</th>\n",
       "      <td>65002</td>\n",
       "      <td>Dejan</td>\n",
       "      <td>B</td>\n",
       "      <td>Dejan Koturovic</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Dejan_Koturović</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  token entity_tag     full_mention  \\\n",
       "65002  65002  Dejan          B  Dejan Koturovic   \n",
       "\n",
       "                                           wiki_url  \n",
       "65002  http://en.wikipedia.org/wiki/Dejan_Koturović  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.id == 65002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-DOCSTART- (947testa CRICKET) CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .  LONDON 1996-08-30  West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .  Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .  After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .  Trailing by 213 , Somerset got a solid start to their second innings before Simmons stepped in to bundle them out for 174 .  Essex , however , look certain to regain their top spot after Nasser Hussain and Peter Such gave them a firm grip on their match against Yorkshire at Headingley .  Hussain , considered surplus to England 's one-day requirements , struck 158 , his first championship century of the season , as Essex reached 372 and took a first innings lead of 82 .  By the close Yorkshire had turned that into a 37-run advantage but off-spinner Such had scuttled their hopes , taking four for 24 in 48 balls and leaving them hanging\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test.token.fillna('', inplace=False).to_list()[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One problem, Simmons does not retrieve the cricket player, but querying for Phil Simmons does.\n",
    "# # TODO: Fix this, one idea is to group by per doc id, check if this token has a better previous full mention\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['simmons'])])\n",
    "\n",
    "# print('____')\n",
    "\n",
    "# display(wiki_items[wiki_items.index.isin(test_dataset.anchor_to_candidate['phil simmons'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a .csv file from id, wiki_url\n",
    "test[['id', 'wiki_url']].to_csv('submission_with_doc_lower_score_limit_sample_skip.csv', index=False)\n",
    "# train[['id', 'wiki_url']].to_csv('train_with_doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
